{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T16:04:08.169070Z","iopub.status.busy":"2023-10-19T16:04:08.168437Z","iopub.status.idle":"2023-10-19T16:05:01.361255Z","shell.execute_reply":"2023-10-19T16:05:01.360154Z","shell.execute_reply.started":"2023-10-19T16:04:08.169043Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.10/site-packages (2.0.8)\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-2.1.0-py3-none-any.whl (774 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.6/774.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.23.5)\n","Requirement already satisfied: torch>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2.0.0)\n","Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.66.1)\n","Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (2023.9.0)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (1.1.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (4.6.3)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_lightning) (0.9.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.4)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pytorch_lightning) (3.0.9)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (3.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n","Installing collected packages: pytorch_lightning\n","  Attempting uninstall: pytorch_lightning\n","    Found existing installation: pytorch-lightning 2.0.8\n","    Uninstalling pytorch-lightning-2.0.8:\n","      Successfully uninstalled pytorch-lightning-2.0.8\n","Successfully installed pytorch_lightning-2.1.0\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\n","Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.9.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.33.0\n","    Uninstalling transformers-4.33.0:\n","      Successfully uninstalled transformers-4.33.0\n","Successfully installed tokenizers-0.14.1 transformers-4.34.1\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Collecting datasets\n","  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Collecting fsspec[http]<2023.9.0,>=2023.1.0 (from datasets)\n","  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: fsspec, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2023.9.0\n","    Uninstalling fsspec-2023.9.0:\n","      Successfully uninstalled fsspec-2023.9.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.2 which is incompatible.\n","distributed 2023.7.1 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.9.0 which is incompatible.\n","s3fs 2023.9.0 requires fsspec==2023.9.0, but you have fsspec 2023.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.14.5 fsspec-2023.6.0\n","Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.1.1)\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.0.0)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.9.0)\n","Requirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (21.3)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.6.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=17.1->lightning-utilities>=0.8.0->torchmetrics) (3.0.9)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Installing collected packages: torchmetrics\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 1.1.1\n","    Uninstalling torchmetrics-1.1.1:\n","      Successfully uninstalled torchmetrics-1.1.1\n","Successfully installed torchmetrics-1.2.0\n"]}],"source":["!pip install torch scikit-learn\n","!pip install --upgrade pytorch_lightning\n","!pip install --upgrade transformers\n","!pip install --upgrade datasets\n","!pip install --upgrade torchmetrics"]},{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-19T16:48:42.209350Z","iopub.status.busy":"2023-10-19T16:48:42.209056Z","iopub.status.idle":"2023-10-19T16:52:20.554182Z","shell.execute_reply":"2023-10-19T16:52:20.553395Z","shell.execute_reply.started":"2023-10-19T16:48:42.209328Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9da7d029d3d24eb986d947cc3379ab06","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ab1021de424a4664be52632349efe0bb","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.061837393790483475    </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mae          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.183049738407135     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mse          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.061837393790483475    </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">          test_r2          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5726782083511353     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.061837393790483475   \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_mae         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.183049738407135    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_mse         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.061837393790483475   \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m         test_r2         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5726782083511353    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'test_loss': 0.061837393790483475,\n","  'test_mse': 0.061837393790483475,\n","  'test_r2': 0.5726782083511353,\n","  'test_mae': 0.183049738407135}]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import pytorch_lightning as pl\n","import torchmetrics\n","import torch\n","import datasets\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, AdamW\n","from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","\n","BATCH_SIZE_TRAIN = 16\n","BATCH_SIZE_EVAL = 16\n","NUM_EPOCH = 5\n","LEARNING_RATE = 2e-5\n","WARM_UP_STEPS = 400\n","\n","# Load the dataset\n","dataset = datasets.load_dataset('google_wellformed_query')\n","\n","# Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n","\n","class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","    \n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","        inputs = tokenizer(item['content'], truncation=True, padding=False, return_tensors='pt')\n","        return {'input_ids': inputs['input_ids'].squeeze(), 'attention_mask': inputs['attention_mask'].squeeze(), 'labels': item['rating']}\n","\n","class CustomModel(pl.LightningModule):\n","    def __init__(self, data_module):\n","        super().__init__()\n","        self.model = AutoModelForSequenceClassification.from_pretrained('distilroberta-base')\n","        self.regression_head = torch.nn.Linear(self.model.config.hidden_size, 1)\n","        self.data_module = data_module\n","        \n","        # Instantiate metrics\n","        self.train_mse = torchmetrics.MeanSquaredError()\n","        self.val_mse = torchmetrics.MeanSquaredError()\n","        self.test_mse = torchmetrics.MeanSquaredError()\n","        \n","        self.train_r2 = torchmetrics.R2Score()\n","        self.val_r2 = torchmetrics.R2Score()\n","        self.test_r2 = torchmetrics.R2Score()\n","        \n","        self.train_mae = torchmetrics.MeanAbsoluteError()\n","        self.val_mae = torchmetrics.MeanAbsoluteError()\n","        self.test_mae = torchmetrics.MeanAbsoluteError()\n","\n","    def forward(self, input_ids, attention_mask, **kwargs):\n","        outputs = self.model.base_model(input_ids=input_ids, attention_mask=attention_mask)\n","        rating = self.regression_head(outputs.last_hidden_state[:, 0, :])\n","        rating = F.sigmoid(rating)\n","        return rating.squeeze()\n","    \n","    def training_step(self, batch, batch_idx):\n","        outputs = self(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n","        loss = torch.nn.functional.mse_loss(outputs, batch['labels'])\n","        self.log('train_loss', loss)\n","        self.train_mse(outputs, batch['labels'])\n","        self.train_r2(outputs, batch['labels'])\n","        self.train_mae(outputs, batch['labels'])\n","        return loss\n","    \n","    def validation_step(self, batch, batch_idx):\n","        outputs = self(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n","        loss = torch.nn.functional.mse_loss(outputs, batch['labels'])\n","        self.log('val_loss', loss)\n","        self.val_mse(outputs, batch['labels'])\n","        self.val_r2(outputs, batch['labels'])\n","        self.val_mae(outputs, batch['labels'])\n","\n","    def test_step(self, batch, batch_idx):\n","        outputs = self(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n","        loss = torch.nn.functional.mse_loss(outputs, batch['labels'])\n","        self.log('test_loss', loss)\n","        self.test_mse(outputs, batch['labels'])\n","        self.test_r2(outputs, batch['labels'])\n","        self.test_mae(outputs, batch['labels'])\n","        return {'test_loss': loss} \n","    \n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=LEARNING_RATE)\n","        steps_per_epoch = len(self.data_module.train_dataset) // BATCH_SIZE_TRAIN\n","        total_steps = steps_per_epoch * NUM_EPOCH\n","        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARM_UP_STEPS, num_training_steps=total_steps)\n","        return [optimizer], [{'scheduler': scheduler, 'interval': 'step'}]\n","\n","    def on_validation_epoch_end(self):\n","        self.log('val_mse', self.val_mse.compute(), prog_bar=True)\n","        self.log('val_r2', self.val_r2.compute(), prog_bar=True)\n","        self.log('val_mae', self.val_mae.compute(), prog_bar=True)\n","        \n","        # don't forget to reset at the end of epoch\n","        self.val_mse.reset()\n","        self.val_r2.reset()\n","        self.val_mae.reset()\n","\n","    def on_test_epoch_end(self):\n","        self.log('test_mse', self.test_mse.compute(), prog_bar=True)\n","        self.log('test_r2', self.test_r2.compute(), prog_bar=True)\n","        self.log('test_mae', self.test_mae.compute(), prog_bar=True)\n","        \n","        # don't forget to reset at the end of epoch\n","        self.test_mse.reset()\n","        self.test_r2.reset()\n","        self.test_mae.reset()\n","    \n","class CustomDataModule(pl.LightningDataModule):\n","    def __init__(self, dataset, tokenizer):\n","        super().__init__()\n","        self.dataset = dataset\n","        self.tokenizer = tokenizer\n","        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n","    \n","    def setup(self, stage=None):\n","        self.train_dataset = CustomDataset(self.dataset['train'])\n","        self.val_dataset = CustomDataset(self.dataset['validation'])\n","        self.test_dataset = CustomDataset(self.dataset['test'])\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True, collate_fn=self.data_collator)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=BATCH_SIZE_EVAL, collate_fn=self.data_collator)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self.test_dataset, batch_size=BATCH_SIZE_EVAL, collate_fn=self.data_collator)\n","\n","# Initialize data module, model, and trainer\n","data_module = CustomDataModule(dataset, tokenizer)\n","model = CustomModel(data_module=data_module)\n","trainer = pl.Trainer(max_epochs=NUM_EPOCH,accelerator=\"auto\")\n","\n","# Train the model\n","trainer.fit(model, data_module)\n","\n","# Evaluate the model on the test dataset\n","trainer.test(datamodule=data_module)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T16:52:37.688817Z","iopub.status.busy":"2023-10-19T16:52:37.688521Z","iopub.status.idle":"2023-10-19T16:52:38.771480Z","shell.execute_reply":"2023-10-19T16:52:38.770607Z","shell.execute_reply.started":"2023-10-19T16:52:37.688795Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving the model...\n"]},{"data":{"text/plain":["('/kaggle/working/new/tokenizer_config.json',\n"," '/kaggle/working/new/special_tokens_map.json',\n"," '/kaggle/working/new/vocab.json',\n"," '/kaggle/working/new/merges.txt',\n"," '/kaggle/working/new/added_tokens.json',\n"," '/kaggle/working/new/tokenizer.json')"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Saving the model...\")\n","model.model.save_pretrained(\"/kaggle/working/new\")\n","\n","# Save the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n","tokenizer_save_path = \"/kaggle/working/new\"\n","tokenizer.save_pretrained(tokenizer_save_path)\n","\n","# Save the regression head\n","torch.save(model.regression_head.state_dict(), f\"/kaggle/working/new/regression_head.pth\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-10-19T16:52:20.555780Z","iopub.status.busy":"2023-10-19T16:52:20.555527Z","iopub.status.idle":"2023-10-19T16:52:20.611096Z","shell.execute_reply":"2023-10-19T16:52:20.610259Z","shell.execute_reply.started":"2023-10-19T16:52:20.555752Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sentence: The cat and dog in the yard.\n","Predicted Rating: 0.2043018490076065\n","\n","Sentence: she don't like apples.\n","Predicted Rating: 0.08289707452058792\n","\n","Sentence: Is rain sunny days sometimes?\n","Predicted Rating: 0.20011107623577118\n","\n","Sentence: She enjoys reading books and playing chess.\n","Predicted Rating: 0.8915352821350098\n","\n","Sentence: How many planets are there in our solar system?\n","Predicted Rating: 0.974799394607544\n","\n"]},{"data":{"text/plain":["\"Sentence: The cat and dog in the yard.\\nPredicted Rating: 0.3482873737812042\\n\\nSentence: she don't like apples.\\nPredicted Rating: 0.07787154614925385\\n\\nSentence: Is rain sunny days sometimes?\\nPredicted Rating: 0.19854165613651276\\n\\nSentence: She enjoys reading books and playing chess.\\nPredicted Rating: 0.9327691793441772\\n\\nSentence: How many planets are there in our solar system?\\nPredicted Rating: 0.9746372103691101\\n\""]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Sentences\n","sentences = [\n","    \"The cat and dog in the yard.\",  # Incorrect - It should be \"The cat and dog are in the yard.\"\n","    \"she don't like apples.\",  # Incorrect - It should be \"She doesn't like apples.\"\n","    \"Is rain sunny days sometimes?\",  # Incorrect - It should be \"Do sunny days sometimes have rain?\"\n","    \"She enjoys reading books and playing chess.\",  # Correct\n","    \"How many planets are there in our solar system?\"  # Correct\n","]\n","\n","# Tokenizing the sentences\n","inputs = tokenizer(sentences, truncation=True, padding=True, return_tensors='pt')\n","\n","# Getting the model's predictions\n","with torch.no_grad():  # Disabling gradient calculation as we are only doing inference\n","    model.eval()  # Setting the model to evaluation mode\n","    predicted_ratings = model(\n","        input_ids=inputs['input_ids'], \n","        attention_mask=inputs['attention_mask']\n","    )\n","\n","# The predicted_ratings is a tensor, so we'll convert it to a list of standard Python numbers\n","predicted_ratings = predicted_ratings.squeeze().tolist()\n","\n","# Printing the predicted ratings\n","for i, rating in enumerate(predicted_ratings):\n","    print(f'Sentence: {sentences[i]}')\n","    print(f'Predicted Rating: {rating}\\n')\n","\n","#Reference\n","'''Sentence: The cat and dog in the yard.\n","Predicted Rating: 0.20430190861225128\n","\n","Sentence: she don't like apples.\n","Predicted Rating: 0.08289700001478195\n","\n","Sentence: Is rain sunny days sometimes?\n","Predicted Rating: 0.20011138916015625\n","\n","Sentence: She enjoys reading books and playing chess.\n","Predicted Rating: 0.8915354013442993\n","\n","Sentence: How many planets are there in our solar system?\n","Predicted Rating: 0.974799394607544\n","'''"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
